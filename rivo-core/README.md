# ğŸ” Module Core (RAG)

Ce module constitue le cÅ“ur de la fonctionnalitÃ© RAG (Retrieval-Augmented Generation) de Mistral-Gagnant. Il gÃ¨re la crÃ©ation d'embeddings, la rÃ©cupÃ©ration de contextes pertinents et l'interaction avec les diffÃ©rents LLMs.

## ğŸ“‹ Structure du module

```
core/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/         # ImplÃ©mentations des agents conversationnels
â”‚   â”œâ”€â”€ chains/         # ChaÃ®nes LangChain pour les diffÃ©rents workflows
â”‚   â”œâ”€â”€ db/             # Connexions aux bases de donnÃ©es (Supabase, Chroma)
â”‚   â”œâ”€â”€ embeddings/     # CrÃ©ation et gestion des embeddings
â”‚   â”œâ”€â”€ llm/            # IntÃ©grations avec les modÃ¨les (Mistral, Ollama)
â”‚   â”œâ”€â”€ loaders/        # Chargeurs de documents (Notion, fichiers locaux)
â”‚   â”œâ”€â”€ memory/         # Gestion de la mÃ©moire des conversations
â”‚   â”œâ”€â”€ types/          # DÃ©finitions de types TypeScript
â”‚   â”œâ”€â”€ utils/          # Fonctions utilitaires
â”‚   â””â”€â”€ index.ts        # Point d'entrÃ©e principal du module
â”œâ”€â”€ tests/             # Tests unitaires et d'intÃ©gration
â””â”€â”€ dist/              # Code compilÃ© (gÃ©nÃ©rÃ©)
```

## âš™ï¸ Configuration

1. **Variables d'environnement**

Copiez le fichier `.env.exemple` en `.env` et configurez les variables suivantes :

```
LANGSMITH_TRACING=true                # Activer le traÃ§age LangSmith (optionnel)
HUGGING_FACE_API_KEY=votre_clÃ©_hf     # ClÃ© API Hugging Face pour les embeddings
LANGSMITH_API_KEY=votre_clÃ©_langsmith # ClÃ© API LangSmith (optionnel)
MISTRAL_API_KEY=votre_clÃ©_mistral     # ClÃ© API Mistral (si utilisation de l'API)
NOTION_API_KEY=votre_clÃ©_notion       # ClÃ© API Notion
NOTION_DATABASE_ID=votre_db_id        # ID de la base de donnÃ©es Notion
SUPABASE_PRIVATE_KEY=votre_clÃ©_supabase # ClÃ© privÃ©e Supabase
SUPABASE_URL=votre_url_supabase       # URL de votre instance Supabase
CHROMA_URL=http://localhost:8000      # URL de votre instance ChromaDB
```

2. **ModÃ¨les Ollama**

Assurez-vous d'avoir les modÃ¨les requis installÃ©s via Ollama :

```bash
ollama pull mistral
ollama pull hermes3
ollama pull cogito
```

## ğŸš€ Utilisation

### En tant que dÃ©pendance

Le module core peut Ãªtre utilisÃ© comme dÃ©pendance dans d'autres projets :

```bash
npm install @mistral-gagnant/core
```

Exemple d'utilisation :

```typescript
import { ChatAgent, NotionLoader, SupabaseVectorStore } from '@mistral-gagnant/core';

// Initialisation de l'agent
const agent = new ChatAgent({
  llmModel: 'mistral',
  vectorStore: new SupabaseVectorStore(),
  documentLoader: new NotionLoader(),
});

// Utilisation de l'agent
const response = await agent.chat("Quelle est la structure du projet Mistral-Gagnant?");
console.log(response);
```

### En dÃ©veloppement local

1. Installer les dÃ©pendances :
```bash
npm install
```

2. Lancer en mode dÃ©veloppement :
```bash
npm run dev
```

3. Construire le package :
```bash
npm run build
```

4. ExÃ©cuter les tests :
```bash
npm test
```

## ğŸ§© FonctionnalitÃ©s principales

### Gestion des embeddings

Le module utilise les modÃ¨les Sentence Transformers via l'API Hugging Face pour gÃ©nÃ©rer des embeddings de haute qualitÃ© pour les documents et les requÃªtes utilisateur.

### IntÃ©gration Notion

L'intÃ©gration avec Notion permet de charger automatiquement vos bases de connaissances et de les rendre disponibles pour le RAG.

### Stockage vectoriel

Le module prend en charge deux types de stockage vectoriel :
- **Supabase** pour un stockage persistant des embeddings et des mÃ©tadonnÃ©es
- **ChromaDB** pour un stockage local et des recherches rapides

### Agents conversationnels

Plusieurs agents sont disponibles :
- **ChatAgent** - Agent de conversation standard avec RAG
- **ReflectionAgent** - Agent amÃ©liorÃ© avec une capacitÃ© de rÃ©flexion sur ses propres rÃ©ponses
- **ToolAgent** - Agent capable d'utiliser des outils externes (recherche web, calculatrice, etc.)

## ğŸ”§ DÃ©veloppement

### Ajouter un nouveau loader

Pour ajouter un nouveau chargeur de documents (par exemple, pour intÃ©grer une nouvelle source de donnÃ©es) :

1. CrÃ©ez une nouvelle classe dans `src/loaders/` qui implÃ©mente l'interface `DocumentLoader`
2. ImplÃ©mentez la mÃ©thode `loadDocuments()` qui renvoie un tableau de `Document`
3. Exportez votre loader depuis `src/loaders/index.ts`

### Ajouter un nouveau modÃ¨le LLM

Pour intÃ©grer un nouveau modÃ¨le de langage :

1. CrÃ©ez un nouveau wrapper dans `src/llm/` qui Ã©tend la classe `BaseLLM`
2. ImplÃ©mentez les mÃ©thodes d'appel et de streaming
3. Ajoutez le modÃ¨le Ã  la factory LLM dans `src/llm/llmFactory.ts`

## ğŸ“Š Ã‰valuation des performances

Des scripts d'Ã©valuation sont disponibles dans `tests/evaluation/` pour mesurer les performances du systÃ¨me RAG sur diffÃ©rents jeux de donnÃ©es et avec diffÃ©rentes configurations. 